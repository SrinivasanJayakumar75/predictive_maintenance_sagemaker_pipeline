{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use the current working directory as the location for SageMaker Python SDK config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import xgboost\n",
    "import sklearn\n",
    "\n",
    "print(f\"Pandas version: {pandas.__version__}\")\n",
    "print(f\"XGBoost version: {xgboost.__version__}\")\n",
    "print(f\"SKLearn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the seaborn data visulization library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from the UCI website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "\n",
    "input_data_dir = 'data/'\n",
    "if not os.path.exists(input_data_dir):\n",
    "    os.makedirs(input_data_dir)\n",
    "input_data_path = os.path.join(input_data_dir, 'predictive_maintenance_raw_data_header.csv')\n",
    "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv\"\n",
    "urllib.request.urlretrieve(dataset_url, input_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the number of samples (rows) and features (columns) in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(input_data_path)\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data types for each column and identify columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the possible values for the \"Machine failure\" column and frequency of their occurence over the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Machine failure'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the target columns to visualise the distribution of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['Machine failure'].value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the count of unique values for colums in df\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the attributes you are not interested in and keep only the numeric attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.sample(frac =.1)\n",
    "df1 = df1.drop(['UDI', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'], axis=1).select_dtypes(include='number')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the summary of the pre-processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a pair plot to spot correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seaborn.pairplot(df1, hue='Machine failure', corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sagemaker.remote_function import remote\n",
    "\n",
    "@remote(keep_alive_period_in_seconds=3600, job_name_prefix=\"amzn-sm-btd-preprocess\")\n",
    "def preprocess(df):\n",
    "    columns = ['Type', 'Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Machine failure']\n",
    "    cat_columns = ['Type']\n",
    "    num_columns = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']\n",
    "    target_column = 'Machine failure'\n",
    "\n",
    "    df = df[columns]\n",
    "\n",
    "    training_ratio = 0.8\n",
    "    validation_ratio = 0.1\n",
    "    test_ratio = 0.1\n",
    "\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "\n",
    "    sm = SMOTE(sampling_strategy = 1, k_neighbors = 3)\n",
    "\n",
    "    X, y = sm.fit_resample(X, y)\n",
    "\n",
    "    print(f'Splitting data training ({training_ratio}), validation ({validation_ratio}), and test ({test_ratio}) sets ')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=0, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_ratio/(validation_ratio+training_ratio), random_state=2, stratify=y_train)\n",
    "\n",
    "    # Apply transformations\n",
    "    transformer = ColumnTransformer(transformers=[('numeric', StandardScaler(), num_columns),\n",
    "                                                  ('categorical', OneHotEncoder(), cat_columns)],\n",
    "                                    remainder='passthrough')\n",
    "    featurizer_model = transformer.fit(X_train)\n",
    "    X_train = featurizer_model.transform(X_train)\n",
    "    X_val = featurizer_model.transform(X_val)\n",
    "\n",
    "    print(f'Shape of train features after preprocessing: {X_train.shape}')\n",
    "    print(f'Shape of validation features after preprocessing: {X_val.shape}')\n",
    "    print(f'Shape of test features after preprocessing: {X_test.shape}')\n",
    "    \n",
    "    y_train = y_train.values.reshape(-1)\n",
    "    y_val = y_val.values.reshape(-1)\n",
    "    \n",
    "    print(f'Shape of train labels after preprocessing: {y_train.shape}')\n",
    "    print(f'Shape of validation labels after preprocessing: {y_val.shape}')\n",
    "    print(f'Shape of test labels after preprocessing: {y_test.shape}')\n",
    "\n",
    "    model_file_path=\"/opt/ml/model/sklearn_model.joblib\"\n",
    "    os.makedirs(os.path.dirname(model_file_path), exist_ok=True)\n",
    "    joblib.dump(featurizer_model, model_file_path)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, featurizer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns multiple values, including the training, validation, and test features and labels, and the featurizer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, featurizer_model = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the featurizer model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X_train).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xgboost\n",
    "import numpy as np\n",
    "\n",
    "from sagemaker.remote_function import remote\n",
    "\n",
    "@remote(keep_alive_period_in_seconds=3600, job_name_prefix=\"amzn-sm-btd-train\")\n",
    "def train(X_train, y_train, X_val, y_val,\n",
    "          eta=0.1, \n",
    "          max_depth=2, \n",
    "          gamma=0.0,\n",
    "          min_child_weight=1,\n",
    "          verbosity=0,\n",
    "          objective='binary:logistic',\n",
    "          eval_metric='auc',\n",
    "          num_boost_round=5):\n",
    "\n",
    "    print('Train features shape: {}'.format(X_train.shape))\n",
    "    print('Train labels shape: {}'.format(y_train.shape))\n",
    "    print('Validation features shape: {}'.format(X_val.shape))\n",
    "    print('Validation labels shape: {}'.format(y_val.shape))\n",
    "\n",
    "    # Creating DMatrix(es)\n",
    "    dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "    dval = xgboost.DMatrix(X_val, label=y_val)\n",
    "    watchlist = [(dtrain, \"train\"), (dval, \"validation\")]\n",
    "\n",
    "    print('')\n",
    "    print (f'===Starting training with max_depth {max_depth}===')\n",
    "\n",
    "    param_dist = {\n",
    "        \"max_depth\": max_depth,\n",
    "        \"eta\": eta,\n",
    "        \"gamma\": gamma,\n",
    "        \"min_child_weight\": min_child_weight,\n",
    "        \"verbosity\": verbosity,\n",
    "        \"objective\": objective,\n",
    "        \"eval_metric\": eval_metric\n",
    "    }\n",
    "\n",
    "    xgb = xgboost.train(\n",
    "        params=param_dist,\n",
    "        dtrain=dtrain,\n",
    "        evals=watchlist,\n",
    "        num_boost_round=num_boost_round)\n",
    "\n",
    "    predictions = xgb.predict(dval)\n",
    "\n",
    "    print (\"Metrics for validation set\")\n",
    "    print('')\n",
    "    print (pd.crosstab(index=y_val, columns=np.round(predictions),\n",
    "                       rownames=['Actuals'], colnames=['Predictions'], margins=True))\n",
    "    print('')\n",
    "\n",
    "    rounded_predict = np.round(predictions)\n",
    "\n",
    "    val_accuracy = accuracy_score(y_val, rounded_predict)\n",
    "    val_precision = precision_score(y_val, rounded_predict)\n",
    "    val_recall = recall_score(y_val, rounded_predict)\n",
    "\n",
    "    print(\"Accuracy Model A: %.2f%%\" % (val_accuracy * 100.0))\n",
    "    print(\"Precision Model A: %.2f\" % (val_precision))\n",
    "    print(\"Recall Model A: %.2f\" % (val_recall))\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    val_auc = roc_auc_score(y_val, predictions)\n",
    "    print(\"Validation AUC A: %.2f\" % (val_auc))\n",
    "\n",
    "    model_file_path=\"/opt/ml/model/xgboost_model.bin\"\n",
    "    os.makedirs(os.path.dirname(model_file_path), exist_ok=True)\n",
    "    xgb.save_model(model_file_path)\n",
    "\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the training function will initiate a SageMaker training job because the function is decoarated with the @remote decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta=0.3\n",
    "max_depth=8\n",
    "\n",
    "booster = train(X_train, y_train, X_val, y_val,\n",
    "              eta=eta, \n",
    "              max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the information about the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the models to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@remote(keep_alive_period_in_seconds=600, job_name_prefix=\"amzn-sm-btd-test\")\n",
    "def test(featurizer_model, booster, X_test, y_test):\n",
    "\n",
    "    X_test = featurizer_model.transform(X_test)\n",
    "    y_test = y_test.values.reshape(-1)\n",
    "\n",
    "    dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "    test_predictions = booster.predict(dtest)\n",
    "    \n",
    "    print (\"===Metrics for Test Set===\")\n",
    "    print('')\n",
    "    print (pd.crosstab(index=y_test, columns=np.round(test_predictions), \n",
    "                                     rownames=['Actuals'], \n",
    "                                     colnames=['Predictions'], \n",
    "                                     margins=True)\n",
    "          )\n",
    "    print('')\n",
    "\n",
    "    rounded_predict = np.round(test_predictions)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, rounded_predict)\n",
    "    precision = precision_score(y_test, rounded_predict)\n",
    "    recall = recall_score(y_test, rounded_predict)\n",
    "    print('')\n",
    "\n",
    "    print(\"Accuracy Model A: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(\"Precision Model A: %.2f\" % (precision))\n",
    "    print(\"Recall Model A: %.2f\" % (recall))\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    auc = roc_auc_score(y_test, test_predictions)\n",
    "    print(\"AUC A: %.2f\" % (auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model using the text features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(featurizer_model, booster, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
